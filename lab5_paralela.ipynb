{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPq/zANRmbRSoyqavY7vu3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaJu502/lab5_paralela/blob/main/lab5_paralela.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio no. 5 Programación Paralela\n",
        "## author: Marco Jurado 20308"
      ],
      "metadata": {
        "id": "Sy9ch0aOm49q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WFD-QvKDmoNI",
        "outputId": "4884bb42-987d-48ed-ee53-fba19d2e0b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "E0mAu6jNoCR0",
        "outputId": "714c5df7-514b-428e-aaf0-48f963c149ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-p662y5w8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-p662y5w8\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XwDYOl8VoEBu",
        "outputId": "ed3ab75f-2af7-475d-8aa1-f1bb4e904f9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GPUVcBQdoFkc",
        "outputId": "1da45a91-f54c-40d8-bacd-46324db9c038"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.10/dist-packages (2022.2.2)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.10/dist-packages (from pycuda) (2023.1.1)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.driver as drv\n",
        "import pycuda.autoinit\n",
        "drv.init()\n",
        "print(\"%d device(s) found.\" % drv.Device.count())\n",
        "for i in range(drv.Device.count()):\n",
        "  dev = drv.Device(i)\n",
        "  print(\"Device #%d: %s\" % (i, dev.name()))\n",
        "  print(\" Compute Capability: %d.%d\" % dev.compute_capability())\n",
        "  print(\" Total Memory: %s GB\" % (dev.total_memory() // (1024 * 1024 * 1024)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9IVA3iPxoHSs",
        "outputId": "3fc28a22-4e3a-4789-f3e6-a1b60a3cc9c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 device(s) found.\n",
            "Device #0: Tesla T4\n",
            " Compute Capability: 7.5\n",
            " Total Memory: 14 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que el ambiente esta listo para ser ejecutado con CUDA procedemos a realizar los ejercicios de esta hoja de trabajo."
      ],
      "metadata": {
        "id": "jZOOQIg7oJ0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name lab5.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "\n",
        "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        c[idx] = a[idx] + b[idx];\n",
        "        printf(\"Thread no. %d: %.2f + %.2f = %.2f\\n\", idx, a[idx], b[idx], c[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 500;  // Tamaño de los vectores\n",
        "    float *h_a, *h_b, *h_c; // Vectores en la CPU\n",
        "    float *d_a, *d_b, *d_c; // Vectores en la GPU\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Alojar memoria en la CPU\n",
        "    h_a = (float *)malloc(size);\n",
        "    h_b = (float *)malloc(size);\n",
        "    h_c = (float *)malloc(size);\n",
        "\n",
        "    if (h_a == nullptr || h_b == nullptr || h_c == nullptr) {\n",
        "        std::cerr << \"Error al alojar memoria en la CPU.\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Inicializar los vectores en la CPU con valores aleatorios\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        h_b[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    // Alojar memoria en la GPU\n",
        "    cudaError_t cudaStatus;\n",
        "    cudaStatus = cudaMalloc(&d_a, size);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al alojar memoria en la GPU para d_a: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "    cudaStatus = cudaMalloc(&d_b, size);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al alojar memoria en la GPU para d_b: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "    cudaStatus = cudaMalloc(&d_c, size);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al alojar memoria en la GPU para d_c: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Copiar datos desde la CPU a la GPU\n",
        "    cudaStatus = cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al copiar datos desde la CPU a la GPU para d_a: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "    cudaStatus = cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al copiar datos desde la CPU a la GPU para d_b: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Configuración de la cuadrícula y bloque\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Lanzar el kernel de CUDA\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
        "\n",
        "    cudaStatus = cudaGetLastError();\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al lanzar el kernel de CUDA: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Copiar el resultado de la GPU a la CPU\n",
        "    cudaStatus = cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al copiar datos desde la GPU a la CPU para d_c: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Imprimir el vector resultante y la suma\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        std::cout << \"Resultado[\" << i << \"]: \" << h_c[i] << std::endl;\n",
        "        sum += h_c[i];\n",
        "    }\n",
        "    std::cout << \"Suma total: \" << sum << std::endl;\n",
        "\n",
        "    // Liberar memoria\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "QEKZ2WIXynYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9b42001e-f45a-4c3e-b4ca-2af5c51417ee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/lab5.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 /content/src/lab5.cu -o \"/content/src/lab5.o\""
      ],
      "metadata": {
        "id": "3IBAxB_Kb0od"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 755 /content/src/lab5.o\n",
        "!/content/src/lab5.o"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1pwAkpgLb2ph",
        "outputId": "d66e537f-3b53-4051-ab35-1a2d83901d95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread no. 480: 0.16 + 0.58 = 0.73\n",
            "Thread no. 481: 0.13 + 0.64 = 0.77\n",
            "Thread no. 482: 0.41 + 0.64 = 1.05\n",
            "Thread no. 483: 0.60 + 0.59 = 1.19\n",
            "Thread no. 484: 0.58 + 0.79 = 1.38\n",
            "Thread no. 485: 0.13 + 0.80 = 0.93\n",
            "Thread no. 486: 0.13 + 0.97 = 1.10\n",
            "Thread no. 487: 0.74 + 0.75 = 1.50\n",
            "Thread no. 488: 0.91 + 0.92 = 1.84\n",
            "Thread no. 489: 0.81 + 0.81 = 1.62\n",
            "Thread no. 490: 0.01 + 0.65 = 0.66\n",
            "Thread no. 491: 0.03 + 0.69 = 0.73\n",
            "Thread no. 492: 0.17 + 0.03 = 0.21\n",
            "Thread no. 493: 0.47 + 0.19 = 0.66\n",
            "Thread no. 494: 0.59 + 0.92 = 1.51\n",
            "Thread no. 495: 0.66 + 0.75 = 1.41\n",
            "Thread no. 496: 0.50 + 0.79 = 1.29\n",
            "Thread no. 497: 0.39 + 0.91 = 1.30\n",
            "Thread no. 498: 0.43 + 0.99 = 1.41\n",
            "Thread no. 499: 0.50 + 0.01 = 0.51\n",
            "Thread no. 64: 0.67 + 0.14 = 0.81\n",
            "Thread no. 65: 0.91 + 0.52 = 1.43\n",
            "Thread no. 66: 0.49 + 0.18 = 0.67\n",
            "Thread no. 67: 0.82 + 0.02 = 0.84\n",
            "Thread no. 68: 0.44 + 0.63 = 1.07\n",
            "Thread no. 69: 0.78 + 0.75 = 1.53\n",
            "Thread no. 70: 0.70 + 0.37 = 1.08\n",
            "Thread no. 71: 0.93 + 0.97 = 1.90\n",
            "Thread no. 72: 0.30 + 0.87 = 1.17\n",
            "Thread no. 73: 0.33 + 0.48 = 0.81\n",
            "Thread no. 74: 0.25 + 0.25 = 0.50\n",
            "Thread no. 75: 0.96 + 0.92 = 1.88\n",
            "Thread no. 76: 0.95 + 0.90 = 1.85\n",
            "Thread no. 77: 0.14 + 0.67 = 0.81\n",
            "Thread no. 78: 0.53 + 0.85 = 1.39\n",
            "Thread no. 79: 0.91 + 0.20 = 1.11\n",
            "Thread no. 80: 0.99 + 0.82 = 1.81\n",
            "Thread no. 81: 0.73 + 0.48 = 1.21\n",
            "Thread no. 82: 0.99 + 0.54 = 1.54\n",
            "Thread no. 83: 0.51 + 0.43 = 0.94\n",
            "Thread no. 84: 0.17 + 0.29 = 0.46\n",
            "Thread no. 85: 0.18 + 0.88 = 1.06\n",
            "Thread no. 86: 0.66 + 0.11 = 0.77\n",
            "Thread no. 87: 0.85 + 0.96 = 1.81\n",
            "Thread no. 88: 0.98 + 0.18 = 1.16\n",
            "Thread no. 89: 0.44 + 0.23 = 0.66\n",
            "Thread no. 90: 0.43 + 0.40 = 0.83\n",
            "Thread no. 91: 0.15 + 0.38 = 0.53\n",
            "Thread no. 92: 0.30 + 0.29 = 0.59\n",
            "Thread no. 93: 0.05 + 0.83 = 0.88\n",
            "Thread no. 94: 0.15 + 0.96 = 1.10\n",
            "Thread no. 95: 0.04 + 0.14 = 0.17\n",
            "Thread no. 352: 0.38 + 0.96 = 1.35\n",
            "Thread no. 353: 0.28 + 0.20 = 0.48\n",
            "Thread no. 354: 0.81 + 0.61 = 1.42\n",
            "Thread no. 355: 0.62 + 0.52 = 1.14\n",
            "Thread no. 356: 0.68 + 0.81 = 1.49\n",
            "Thread no. 357: 0.10 + 0.73 = 0.83\n",
            "Thread no. 358: 0.79 + 0.66 = 1.45\n",
            "Thread no. 359: 0.30 + 0.17 = 0.48\n",
            "Thread no. 360: 0.85 + 0.53 = 1.38\n",
            "Thread no. 361: 0.25 + 0.25 = 0.49\n",
            "Thread no. 362: 0.16 + 0.08 = 0.24\n",
            "Thread no. 363: 0.32 + 0.71 = 1.03\n",
            "Thread no. 364: 0.07 + 0.06 = 0.13\n",
            "Thread no. 365: 0.53 + 0.36 = 0.89\n",
            "Thread no. 366: 0.32 + 0.63 = 0.95\n",
            "Thread no. 367: 0.79 + 0.71 = 1.49\n",
            "Thread no. 368: 0.59 + 0.07 = 0.66\n",
            "Thread no. 369: 0.91 + 0.40 = 1.31\n",
            "Thread no. 370: 0.68 + 0.53 = 1.21\n",
            "Thread no. 371: 0.92 + 0.36 = 1.28\n",
            "Thread no. 372: 0.34 + 0.02 = 0.36\n",
            "Thread no. 373: 0.09 + 0.14 = 0.23\n",
            "Thread no. 374: 0.68 + 0.39 = 1.07\n",
            "Thread no. 375: 0.31 + 0.53 = 0.84\n",
            "Thread no. 376: 0.93 + 0.55 = 1.48\n",
            "Thread no. 377: 0.78 + 0.08 = 0.86\n",
            "Thread no. 378: 0.63 + 0.09 = 0.73\n",
            "Thread no. 379: 0.80 + 0.70 = 1.50\n",
            "Thread no. 380: 0.16 + 0.33 = 0.49\n",
            "Thread no. 381: 0.06 + 0.48 = 0.54\n",
            "Thread no. 382: 0.96 + 0.84 = 1.80\n",
            "Thread no. 383: 0.19 + 0.55 = 0.74\n",
            "Thread no. 192: 0.57 + 0.25 = 0.83\n",
            "Thread no. 193: 0.36 + 0.55 = 0.92\n",
            "Thread no. 194: 0.14 + 0.52 = 0.66\n",
            "Thread no. 195: 0.08 + 0.98 = 1.06\n",
            "Thread no. 196: 0.88 + 0.45 = 1.33\n",
            "Thread no. 197: 0.47 + 0.70 = 1.17\n",
            "Thread no. 198: 0.84 + 0.86 = 1.71\n",
            "Thread no. 199: 0.12 + 0.19 = 0.30\n",
            "Thread no. 200: 0.45 + 0.30 = 0.75\n",
            "Thread no. 201: 0.08 + 0.34 = 0.42\n",
            "Thread no. 202: 0.61 + 0.07 = 0.67\n",
            "Thread no. 203: 0.32 + 0.80 = 1.13\n",
            "Thread no. 204: 0.26 + 0.86 = 1.13\n",
            "Thread no. 205: 0.02 + 0.08 = 0.10\n",
            "Thread no. 206: 0.66 + 0.88 = 1.54\n",
            "Thread no. 207: 0.09 + 0.24 = 0.33\n",
            "Thread no. 208: 0.13 + 0.46 = 0.59\n",
            "Thread no. 209: 0.79 + 0.27 = 1.06\n",
            "Thread no. 210: 0.98 + 0.87 = 1.85\n",
            "Thread no. 211: 0.24 + 0.85 = 1.10\n",
            "Thread no. 212: 0.32 + 0.71 = 1.04\n",
            "Thread no. 213: 0.56 + 0.16 = 0.72\n",
            "Thread no. 214: 0.58 + 0.67 = 1.25\n",
            "Thread no. 215: 0.35 + 0.03 = 0.38\n",
            "Thread no. 216: 0.97 + 0.43 = 1.40\n",
            "Thread no. 217: 0.37 + 0.58 = 0.95\n",
            "Thread no. 218: 0.50 + 0.69 = 1.19\n",
            "Thread no. 219: 0.38 + 0.76 = 1.14\n",
            "Thread no. 220: 0.56 + 0.41 = 0.96\n",
            "Thread no. 221: 0.84 + 0.22 = 1.06\n",
            "Thread no. 222: 0.29 + 0.93 = 1.22\n",
            "Thread no. 223: 0.45 + 0.42 = 0.87\n",
            "Thread no. 320: 0.01 + 0.84 = 0.85\n",
            "Thread no. 321: 0.61 + 0.60 = 1.20\n",
            "Thread no. 322: 0.71 + 0.69 = 1.40\n",
            "Thread no. 323: 0.97 + 0.48 = 1.46\n",
            "Thread no. 324: 0.60 + 0.86 = 1.46\n",
            "Thread no. 325: 0.73 + 0.77 = 1.51\n",
            "Thread no. 326: 0.88 + 0.98 = 1.86\n",
            "Thread no. 327: 0.79 + 0.97 = 1.76\n",
            "Thread no. 328: 0.52 + 0.40 = 0.92\n",
            "Thread no. 329: 0.63 + 0.66 = 1.29\n",
            "Thread no. 330: 0.69 + 0.20 = 0.90\n",
            "Thread no. 331: 0.40 + 0.76 = 1.16\n",
            "Thread no. 332: 0.68 + 0.93 = 1.61\n",
            "Thread no. 333: 0.16 + 0.67 = 0.83\n",
            "Thread no. 334: 0.27 + 0.30 = 0.57\n",
            "Thread no. 335: 0.52 + 0.27 = 0.79\n",
            "Thread no. 336: 0.14 + 0.12 = 0.26\n",
            "Thread no. 337: 0.87 + 0.85 = 1.72\n",
            "Thread no. 338: 0.82 + 0.84 = 1.66\n",
            "Thread no. 339: 0.33 + 0.42 = 0.75\n",
            "Thread no. 340: 0.71 + 0.07 = 0.77\n",
            "Thread no. 341: 0.19 + 0.59 = 0.78\n",
            "Thread no. 342: 0.05 + 0.98 = 1.03\n",
            "Thread no. 343: 0.56 + 0.57 = 1.13\n",
            "Thread no. 344: 0.38 + 0.19 = 0.57\n",
            "Thread no. 345: 0.23 + 0.07 = 0.30\n",
            "Thread no. 346: 0.39 + 0.63 = 1.02\n",
            "Thread no. 347: 0.83 + 0.07 = 0.90\n",
            "Thread no. 348: 0.56 + 0.99 = 1.55\n",
            "Thread no. 349: 0.74 + 0.82 = 1.57\n",
            "Thread no. 350: 0.29 + 0.26 = 0.55\n",
            "Thread no. 351: 0.09 + 0.43 = 0.52\n",
            "Thread no. 384: 0.91 + 0.10 = 1.01\n",
            "Thread no. 385: 0.96 + 0.59 = 1.54\n",
            "Thread no. 386: 0.63 + 0.87 = 1.50\n",
            "Thread no. 387: 0.95 + 0.97 = 1.92\n",
            "Thread no. 388: 0.89 + 0.04 = 0.93\n",
            "Thread no. 389: 0.11 + 0.57 = 0.68\n",
            "Thread no. 390: 0.43 + 0.41 = 0.85\n",
            "Thread no. 391: 0.10 + 0.36 = 0.46\n",
            "Thread no. 392: 0.97 + 0.88 = 1.85\n",
            "Thread no. 393: 0.44 + 0.60 = 1.04\n",
            "Thread no. 394: 0.98 + 0.24 = 1.21\n",
            "Thread no. 395: 0.30 + 0.13 = 0.43\n",
            "Thread no. 396: 0.57 + 0.36 = 0.92\n",
            "Thread no. 397: 0.61 + 0.53 = 1.14\n",
            "Thread no. 398: 0.20 + 0.80 = 1.00\n",
            "Thread no. 399: 0.08 + 0.11 = 0.19\n",
            "Thread no. 400: 0.89 + 0.04 = 0.93\n",
            "Thread no. 401: 0.70 + 0.52 = 1.22\n",
            "Thread no. 402: 0.91 + 0.65 = 1.56\n",
            "Thread no. 403: 0.49 + 0.80 = 1.29\n",
            "Thread no. 404: 0.69 + 0.60 = 1.29\n",
            "Thread no. 405: 0.37 + 0.12 = 0.49\n",
            "Thread no. 406: 0.01 + 0.48 = 0.49\n",
            "Thread no. 407: 0.48 + 0.98 = 1.46\n",
            "Thread no. 408: 0.36 + 0.91 = 1.27\n",
            "Thread no. 409: 0.58 + 0.33 = 0.91\n",
            "Thread no. 410: 0.15 + 0.88 = 1.03\n",
            "Thread no. 411: 0.47 + 0.72 = 1.18\n",
            "Thread no. 412: 0.24 + 0.08 = 0.32\n",
            "Thread no. 413: 0.24 + 0.44 = 0.69\n",
            "Thread no. 414: 0.88 + 0.32 = 1.20\n",
            "Thread no. 415: 0.56 + 0.77 = 1.33\n",
            "Thread no. 224: 0.39 + 0.24 = 0.63\n",
            "Thread no. 225: 0.69 + 0.37 = 1.05\n",
            "Thread no. 226: 0.12 + 0.93 = 1.05\n",
            "Thread no. 227: 0.22 + 0.44 = 0.66\n",
            "Thread no. 228: 0.65 + 0.78 = 1.42\n",
            "Thread no. 229: 0.60 + 0.22 = 0.83\n",
            "Thread no. 230: 0.45 + 0.95 = 1.40\n",
            "Thread no. 231: 0.26 + 0.42 = 0.68\n",
            "Thread no. 232: 0.38 + 0.62 = 1.01\n",
            "Thread no. 233: 0.00 + 0.88 = 0.88\n",
            "Thread no. 234: 0.32 + 0.39 = 0.70\n",
            "Thread no. 235: 0.64 + 0.87 = 1.51\n",
            "Thread no. 236: 0.79 + 0.48 = 1.27\n",
            "Thread no. 237: 0.09 + 0.08 = 0.17\n",
            "Thread no. 238: 0.41 + 0.54 = 0.95\n",
            "Thread no. 239: 0.50 + 0.80 = 1.30\n",
            "Thread no. 240: 0.79 + 0.18 = 0.97\n",
            "Thread no. 241: 0.17 + 0.90 = 1.07\n",
            "Thread no. 242: 0.11 + 0.39 = 0.50\n",
            "Thread no. 243: 0.34 + 0.76 = 1.10\n",
            "Thread no. 244: 0.17 + 0.94 = 1.11\n",
            "Thread no. 245: 0.99 + 0.62 = 1.60\n",
            "Thread no. 246: 0.89 + 0.24 = 1.13\n",
            "Thread no. 247: 0.04 + 0.27 = 0.31\n",
            "Thread no. 248: 0.87 + 0.04 = 0.91\n",
            "Thread no. 249: 0.15 + 0.18 = 0.33\n",
            "Thread no. 250: 0.43 + 0.79 = 1.22\n",
            "Thread no. 251: 0.05 + 0.22 = 0.28\n",
            "Thread no. 252: 0.27 + 0.14 = 0.41\n",
            "Thread no. 253: 0.30 + 0.68 = 0.98\n",
            "Thread no. 254: 0.68 + 0.80 = 1.48\n",
            "Thread no. 255: 0.48 + 0.47 = 0.95\n",
            "Thread no. 288: 0.41 + 0.33 = 0.74\n",
            "Thread no. 289: 0.87 + 0.96 = 1.83\n",
            "Thread no. 290: 0.91 + 0.92 = 1.83\n",
            "Thread no. 291: 0.16 + 0.32 = 0.49\n",
            "Thread no. 292: 0.79 + 0.91 = 1.70\n",
            "Thread no. 293: 0.82 + 0.51 = 1.33\n",
            "Thread no. 294: 0.47 + 0.27 = 0.74\n",
            "Thread no. 295: 0.13 + 0.00 = 0.13\n",
            "Thread no. 296: 0.84 + 0.07 = 0.91\n",
            "Thread no. 297: 0.29 + 0.60 = 0.89\n",
            "Thread no. 298: 0.57 + 0.60 = 1.17\n",
            "Thread no. 299: 0.68 + 0.91 = 1.59\n",
            "Thread no. 300: 0.61 + 0.77 = 1.38\n",
            "Thread no. 301: 0.90 + 0.79 = 1.70\n",
            "Thread no. 302: 0.33 + 0.52 = 0.85\n",
            "Thread no. 303: 0.80 + 0.74 = 1.54\n",
            "Thread no. 304: 0.85 + 0.67 = 1.52\n",
            "Thread no. 305: 0.70 + 0.76 = 1.46\n",
            "Thread no. 306: 0.59 + 0.87 = 1.46\n",
            "Thread no. 307: 0.09 + 0.38 = 0.46\n",
            "Thread no. 308: 0.78 + 0.91 = 1.68\n",
            "Thread no. 309: 0.89 + 0.25 = 1.14\n",
            "Thread no. 310: 0.17 + 0.02 = 0.19\n",
            "Thread no. 311: 0.25 + 0.01 = 0.26\n",
            "Thread no. 312: 0.09 + 0.54 = 0.63\n",
            "Thread no. 313: 0.61 + 0.66 = 1.27\n",
            "Thread no. 314: 0.13 + 0.29 = 0.43\n",
            "Thread no. 315: 0.57 + 0.74 = 1.31\n",
            "Thread no. 316: 0.06 + 0.48 = 0.54\n",
            "Thread no. 317: 0.53 + 0.40 = 0.93\n",
            "Thread no. 318: 0.99 + 0.34 = 1.33\n",
            "Thread no. 319: 0.14 + 0.85 = 0.99\n",
            "Thread no. 96: 0.77 + 0.76 = 1.53\n",
            "Thread no. 97: 0.62 + 0.76 = 1.38\n",
            "Thread no. 98: 0.31 + 0.12 = 0.43\n",
            "Thread no. 99: 0.20 + 0.48 = 0.67\n",
            "Thread no. 100: 0.41 + 0.38 = 0.79\n",
            "Thread no. 101: 0.35 + 0.07 = 0.42\n",
            "Thread no. 102: 0.49 + 0.20 = 0.69\n",
            "Thread no. 103: 0.03 + 0.47 = 0.50\n",
            "Thread no. 104: 0.38 + 0.47 = 0.85\n",
            "Thread no. 105: 0.70 + 0.81 = 1.51\n",
            "Thread no. 106: 0.87 + 0.85 = 1.71\n",
            "Thread no. 107: 0.19 + 0.17 = 0.35\n",
            "Thread no. 108: 0.14 + 0.24 = 0.38\n",
            "Thread no. 109: 1.00 + 0.28 = 1.28\n",
            "Thread no. 110: 0.19 + 0.03 = 0.23\n",
            "Thread no. 111: 0.42 + 0.97 = 1.39\n",
            "Thread no. 112: 0.79 + 0.04 = 0.83\n",
            "Thread no. 113: 0.73 + 0.10 = 0.83\n",
            "Thread no. 114: 0.16 + 0.93 = 1.09\n",
            "Thread no. 115: 0.58 + 0.57 = 1.15\n",
            "Thread no. 116: 0.31 + 0.93 = 1.24\n",
            "Thread no. 117: 0.64 + 0.80 = 1.44\n",
            "Thread no. 118: 0.13 + 0.67 = 0.80\n",
            "Thread no. 119: 0.27 + 0.51 = 0.78\n",
            "Thread no. 120: 0.13 + 0.97 = 1.10\n",
            "Thread no. 121: 0.32 + 1.00 = 1.32\n",
            "Thread no. 122: 0.82 + 0.50 = 1.32\n",
            "Thread no. 123: 0.16 + 0.96 = 1.12\n",
            "Thread no. 124: 0.74 + 0.16 = 0.90\n",
            "Thread no. 125: 0.24 + 0.93 = 1.18\n",
            "Thread no. 126: 0.20 + 0.66 = 0.86\n",
            "Thread no. 127: 0.90 + 0.99 = 1.89\n",
            "Thread no. 0: 0.23 + 0.00 = 0.24\n",
            "Thread no. 1: 0.33 + 0.25 = 0.58\n",
            "Thread no. 2: 0.03 + 0.26 = 0.29\n",
            "Thread no. 3: 0.41 + 0.34 = 0.75\n",
            "Thread no. 4: 0.03 + 0.78 = 0.80\n",
            "Thread no. 5: 0.18 + 0.25 = 0.43\n",
            "Thread no. 6: 0.96 + 0.43 = 1.38\n",
            "Thread no. 7: 0.79 + 0.66 = 1.45\n",
            "Thread no. 8: 0.68 + 0.57 = 1.25\n",
            "Thread no. 9: 0.74 + 0.18 = 0.92\n",
            "Thread no. 10: 0.89 + 0.10 = 0.99\n",
            "Thread no. 11: 0.98 + 0.40 = 1.38\n",
            "Thread no. 12: 0.77 + 0.55 = 1.32\n",
            "Thread no. 13: 0.16 + 0.18 = 0.35\n",
            "Thread no. 14: 0.78 + 0.08 = 0.86\n",
            "Thread no. 15: 0.38 + 0.01 = 0.39\n",
            "Thread no. 16: 0.08 + 0.71 = 0.80\n",
            "Thread no. 17: 0.25 + 0.12 = 0.37\n",
            "Thread no. 18: 0.97 + 0.66 = 1.64\n",
            "Thread no. 19: 0.46 + 1.00 = 1.46\n",
            "Thread no. 20: 0.44 + 0.64 = 1.08\n",
            "Thread no. 21: 0.25 + 0.40 = 0.65\n",
            "Thread no. 22: 0.07 + 0.04 = 0.11\n",
            "Thread no. 23: 0.06 + 0.75 = 0.81\n",
            "Thread no. 24: 0.61 + 0.80 = 1.42\n",
            "Thread no. 25: 0.93 + 0.51 = 1.43\n",
            "Thread no. 26: 0.91 + 0.90 = 1.81\n",
            "Thread no. 27: 0.90 + 0.67 = 1.58\n",
            "Thread no. 28: 0.46 + 0.07 = 0.52\n",
            "Thread no. 29: 0.86 + 0.23 = 1.09\n",
            "Thread no. 30: 0.15 + 0.24 = 0.39\n",
            "Thread no. 31: 0.24 + 0.23 = 0.47\n",
            "Thread no. 448: 0.58 + 0.97 = 1.55\n",
            "Thread no. 449: 0.04 + 0.46 = 0.49\n",
            "Thread no. 450: 0.20 + 0.89 = 1.09\n",
            "Thread no. 451: 0.71 + 0.86 = 1.56\n",
            "Thread no. 452: 0.47 + 0.35 = 0.81\n",
            "Thread no. 453: 0.43 + 0.24 = 0.68\n",
            "Thread no. 454: 0.35 + 0.28 = 0.63\n",
            "Thread no. 455: 0.14 + 0.31 = 0.45\n",
            "Thread no. 456: 0.72 + 0.29 = 1.01\n",
            "Thread no. 457: 0.10 + 0.24 = 0.34\n",
            "Thread no. 458: 0.43 + 0.95 = 1.38\n",
            "Thread no. 459: 0.91 + 0.79 = 1.69\n",
            "Thread no. 460: 0.33 + 0.60 = 0.93\n",
            "Thread no. 461: 0.69 + 0.78 = 1.46\n",
            "Thread no. 462: 0.09 + 0.49 = 0.58\n",
            "Thread no. 463: 0.56 + 0.67 = 1.23\n",
            "Thread no. 464: 0.46 + 0.60 = 1.05\n",
            "Thread no. 465: 0.13 + 0.65 = 0.78\n",
            "Thread no. 466: 0.49 + 0.84 = 1.32\n",
            "Thread no. 467: 0.51 + 0.95 = 1.46\n",
            "Thread no. 468: 0.18 + 0.94 = 1.12\n",
            "Thread no. 469: 0.20 + 0.53 = 0.73\n",
            "Thread no. 470: 0.22 + 0.33 = 0.55\n",
            "Thread no. 471: 0.84 + 0.94 = 1.79\n",
            "Thread no. 472: 0.63 + 0.94 = 1.57\n",
            "Thread no. 473: 0.18 + 0.05 = 0.23\n",
            "Thread no. 474: 0.90 + 0.09 = 0.98\n",
            "Thread no. 475: 0.84 + 0.22 = 1.06\n",
            "Thread no. 476: 0.68 + 0.53 = 1.21\n",
            "Thread no. 477: 1.00 + 0.78 = 1.78\n",
            "Thread no. 478: 0.02 + 0.56 = 0.58\n",
            "Thread no. 479: 0.45 + 0.47 = 0.92\n",
            "Thread no. 128: 0.70 + 0.63 = 1.33\n",
            "Thread no. 129: 0.09 + 0.86 = 0.95\n",
            "Thread no. 130: 0.56 + 0.67 = 1.23\n",
            "Thread no. 131: 0.43 + 0.87 = 1.30\n",
            "Thread no. 132: 0.60 + 0.07 = 0.67\n",
            "Thread no. 133: 0.67 + 0.73 = 1.39\n",
            "Thread no. 134: 0.74 + 0.94 = 1.67\n",
            "Thread no. 135: 0.23 + 0.87 = 1.10\n",
            "Thread no. 136: 0.91 + 0.55 = 1.46\n",
            "Thread no. 137: 0.87 + 0.73 = 1.60\n",
            "Thread no. 138: 0.05 + 0.04 = 0.09\n",
            "Thread no. 139: 0.68 + 0.79 = 1.48\n",
            "Thread no. 140: 0.20 + 0.93 = 1.12\n",
            "Thread no. 141: 0.73 + 0.39 = 1.12\n",
            "Thread no. 142: 0.59 + 0.63 = 1.22\n",
            "Thread no. 143: 0.38 + 0.29 = 0.67\n",
            "Thread no. 144: 0.26 + 0.47 = 0.73\n",
            "Thread no. 145: 0.15 + 0.82 = 0.97\n",
            "Thread no. 146: 0.14 + 0.59 = 0.73\n",
            "Thread no. 147: 0.68 + 0.74 = 1.42\n",
            "Thread no. 148: 0.66 + 0.35 = 1.00\n",
            "Thread no. 149: 0.46 + 0.39 = 0.86\n",
            "Thread no. 150: 0.29 + 0.70 = 0.98\n",
            "Thread no. 151: 0.26 + 0.19 = 0.46\n",
            "Thread no. 152: 0.25 + 0.13 = 0.38\n",
            "Thread no. 153: 0.92 + 0.30 = 1.22\n",
            "Thread no. 154: 0.17 + 0.61 = 0.77\n",
            "Thread no. 155: 0.09 + 0.36 = 0.46\n",
            "Thread no. 156: 0.53 + 0.82 = 1.35\n",
            "Thread no. 157: 0.76 + 0.12 = 0.88\n",
            "Thread no. 158: 0.45 + 0.14 = 0.59\n",
            "Thread no. 159: 0.42 + 0.70 = 1.12\n",
            "Thread no. 32: 0.96 + 0.49 = 1.45\n",
            "Thread no. 33: 0.35 + 0.93 = 1.28\n",
            "Thread no. 34: 0.16 + 0.81 = 0.96\n",
            "Thread no. 35: 0.93 + 0.60 = 1.53\n",
            "Thread no. 36: 0.45 + 0.18 = 0.63\n",
            "Thread no. 37: 0.99 + 0.51 = 1.51\n",
            "Thread no. 38: 0.22 + 0.05 = 0.28\n",
            "Thread no. 39: 0.26 + 0.83 = 1.10\n",
            "Thread no. 40: 0.86 + 0.19 = 1.05\n",
            "Thread no. 41: 0.34 + 0.77 = 1.11\n",
            "Thread no. 42: 0.10 + 0.24 = 0.34\n",
            "Thread no. 43: 0.44 + 0.55 = 0.99\n",
            "Thread no. 44: 0.31 + 0.30 = 0.61\n",
            "Thread no. 45: 0.78 + 0.46 = 1.24\n",
            "Thread no. 46: 0.54 + 0.02 = 0.56\n",
            "Thread no. 47: 0.69 + 0.50 = 1.19\n",
            "Thread no. 48: 0.52 + 0.04 = 0.56\n",
            "Thread no. 49: 0.42 + 0.67 = 1.10\n",
            "Thread no. 50: 0.85 + 0.35 = 1.20\n",
            "Thread no. 51: 0.27 + 0.30 = 0.56\n",
            "Thread no. 52: 0.53 + 0.26 = 0.80\n",
            "Thread no. 53: 0.81 + 0.76 = 1.57\n",
            "Thread no. 54: 0.32 + 0.08 = 0.39\n",
            "Thread no. 55: 0.59 + 0.17 = 0.77\n",
            "Thread no. 56: 0.27 + 0.93 = 1.20\n",
            "Thread no. 57: 0.94 + 0.36 = 1.30\n",
            "Thread no. 58: 0.17 + 0.38 = 0.55\n",
            "Thread no. 59: 0.92 + 0.48 = 1.40\n",
            "Thread no. 60: 0.68 + 0.70 = 1.38\n",
            "Thread no. 61: 0.94 + 0.22 = 1.16\n",
            "Thread no. 62: 0.72 + 0.63 = 1.35\n",
            "Thread no. 63: 0.71 + 0.24 = 0.95\n",
            "Thread no. 160: 0.61 + 0.57 = 1.18\n",
            "Thread no. 161: 0.52 + 0.75 = 1.27\n",
            "Thread no. 162: 0.15 + 0.20 = 0.36\n",
            "Thread no. 163: 0.49 + 0.81 = 1.30\n",
            "Thread no. 164: 0.55 + 0.96 = 1.51\n",
            "Thread no. 165: 0.20 + 0.84 = 1.04\n",
            "Thread no. 166: 0.65 + 0.46 = 1.12\n",
            "Thread no. 167: 0.03 + 0.90 = 0.93\n",
            "Thread no. 168: 0.60 + 0.95 = 1.54\n",
            "Thread no. 169: 0.20 + 0.76 = 0.96\n",
            "Thread no. 170: 0.55 + 0.29 = 0.84\n",
            "Thread no. 171: 0.13 + 0.09 = 0.21\n",
            "Thread no. 172: 0.11 + 0.89 = 0.99\n",
            "Thread no. 173: 0.21 + 0.55 = 0.76\n",
            "Thread no. 174: 0.03 + 0.63 = 0.65\n",
            "Thread no. 175: 0.26 + 0.64 = 0.90\n",
            "Thread no. 176: 0.19 + 0.78 = 0.97\n",
            "Thread no. 177: 0.39 + 0.35 = 0.74\n",
            "Thread no. 178: 0.98 + 0.88 = 1.86\n",
            "Thread no. 179: 0.16 + 0.53 = 0.69\n",
            "Thread no. 180: 0.84 + 0.36 = 1.20\n",
            "Thread no. 181: 0.36 + 0.49 = 0.86\n",
            "Thread no. 182: 0.82 + 0.39 = 1.22\n",
            "Thread no. 183: 0.39 + 0.42 = 0.81\n",
            "Thread no. 184: 0.34 + 0.59 = 0.93\n",
            "Thread no. 185: 0.18 + 0.90 = 1.08\n",
            "Thread no. 186: 0.88 + 0.31 = 1.19\n",
            "Thread no. 187: 0.98 + 0.99 = 1.97\n",
            "Thread no. 188: 0.19 + 0.19 = 0.39\n",
            "Thread no. 189: 0.54 + 0.22 = 0.76\n",
            "Thread no. 190: 0.82 + 0.80 = 1.62\n",
            "Thread no. 191: 0.86 + 0.01 = 0.87\n",
            "Thread no. 256: 0.98 + 0.64 = 1.62\n",
            "Thread no. 257: 0.37 + 0.09 = 0.46\n",
            "Thread no. 258: 0.03 + 0.71 = 0.74\n",
            "Thread no. 259: 0.85 + 0.20 = 1.05\n",
            "Thread no. 260: 0.65 + 0.84 = 1.49\n",
            "Thread no. 261: 0.81 + 0.54 = 1.36\n",
            "Thread no. 262: 0.08 + 0.85 = 0.94\n",
            "Thread no. 263: 0.82 + 0.95 = 1.76\n",
            "Thread no. 264: 0.90 + 0.97 = 1.86\n",
            "Thread no. 265: 0.13 + 0.33 = 0.46\n",
            "Thread no. 266: 0.76 + 0.18 = 0.94\n",
            "Thread no. 267: 0.55 + 0.02 = 0.57\n",
            "Thread no. 268: 0.32 + 0.85 = 1.17\n",
            "Thread no. 269: 0.70 + 0.01 = 0.71\n",
            "Thread no. 270: 0.65 + 0.18 = 0.82\n",
            "Thread no. 271: 0.47 + 0.63 = 1.10\n",
            "Thread no. 272: 0.82 + 0.84 = 1.67\n",
            "Thread no. 273: 0.72 + 0.86 = 1.58\n",
            "Thread no. 274: 0.55 + 0.57 = 1.13\n",
            "Thread no. 275: 0.05 + 0.20 = 0.26\n",
            "Thread no. 276: 0.41 + 0.87 = 1.28\n",
            "Thread no. 277: 0.75 + 0.50 = 1.24\n",
            "Thread no. 278: 0.72 + 0.56 = 1.28\n",
            "Thread no. 279: 0.44 + 0.62 = 1.06\n",
            "Thread no. 280: 0.53 + 0.58 = 1.10\n",
            "Thread no. 281: 0.95 + 0.29 = 1.23\n",
            "Thread no. 282: 0.76 + 0.49 = 1.25\n",
            "Thread no. 283: 0.31 + 0.08 = 0.39\n",
            "Thread no. 284: 0.34 + 0.01 = 0.35\n",
            "Thread no. 285: 0.09 + 0.99 = 1.08\n",
            "Thread no. 286: 0.19 + 0.56 = 0.75\n",
            "Thread no. 287: 0.62 + 0.01 = 0.63\n",
            "Thread no. 416: 0.36 + 0.26 = 0.62\n",
            "Thread no. 417: 0.29 + 0.26 = 0.56\n",
            "Thread no. 418: 0.91 + 0.78 = 1.70\n",
            "Thread no. 419: 0.06 + 0.60 = 0.67\n",
            "Thread no. 420: 0.38 + 0.43 = 0.81\n",
            "Thread no. 421: 0.72 + 0.39 = 1.11\n",
            "Thread no. 422: 0.91 + 0.20 = 1.11\n",
            "Thread no. 423: 0.37 + 0.27 = 0.64\n",
            "Thread no. 424: 0.11 + 0.95 = 1.06\n",
            "Thread no. 425: 0.60 + 0.26 = 0.87\n",
            "Thread no. 426: 0.83 + 0.07 = 0.90\n",
            "Thread no. 427: 0.98 + 0.07 = 1.05\n",
            "Thread no. 428: 0.15 + 0.22 = 0.37\n",
            "Thread no. 429: 0.52 + 0.03 = 0.54\n",
            "Thread no. 430: 0.54 + 0.07 = 0.62\n",
            "Thread no. 431: 0.80 + 0.90 = 1.70\n",
            "Thread no. 432: 0.34 + 0.09 = 0.43\n",
            "Thread no. 433: 0.16 + 0.25 = 0.42\n",
            "Thread no. 434: 0.87 + 0.23 = 1.10\n",
            "Thread no. 435: 0.85 + 0.25 = 1.11\n",
            "Thread no. 436: 0.66 + 0.58 = 1.24\n",
            "Thread no. 437: 0.64 + 0.57 = 1.21\n",
            "Thread no. 438: 0.78 + 0.01 = 0.79\n",
            "Thread no. 439: 0.84 + 0.89 = 1.73\n",
            "Thread no. 440: 0.96 + 0.45 = 1.40\n",
            "Thread no. 441: 0.16 + 0.79 = 0.94\n",
            "Thread no. 442: 0.52 + 0.14 = 0.65\n",
            "Thread no. 443: 0.86 + 0.67 = 1.52\n",
            "Thread no. 444: 0.36 + 0.37 = 0.73\n",
            "Thread no. 445: 0.69 + 0.90 = 1.60\n",
            "Thread no. 446: 0.45 + 0.49 = 0.94\n",
            "Thread no. 447: 0.80 + 0.78 = 1.59\n",
            "Resultado[0]: 0.236041\n",
            "Resultado[1]: 0.57763\n",
            "Resultado[2]: 0.292986\n",
            "Resultado[3]: 0.74704\n",
            "Resultado[4]: 0.803769\n",
            "Resultado[5]: 0.4349\n",
            "Resultado[6]: 1.3828\n",
            "Resultado[7]: 1.45375\n",
            "Resultado[8]: 1.25471\n",
            "Resultado[9]: 0.921293\n",
            "Resultado[10]: 0.99475\n",
            "Resultado[11]: 1.37524\n",
            "Resultado[12]: 1.31868\n",
            "Resultado[13]: 0.347232\n",
            "Resultado[14]: 0.856801\n",
            "Resultado[15]: 0.390734\n",
            "Resultado[16]: 0.799258\n",
            "Resultado[17]: 0.372702\n",
            "Resultado[18]: 1.63663\n",
            "Resultado[19]: 1.45648\n",
            "Resultado[20]: 1.08015\n",
            "Resultado[21]: 0.648341\n",
            "Resultado[22]: 0.107972\n",
            "Resultado[23]: 0.811033\n",
            "Resultado[24]: 1.4171\n",
            "Resultado[25]: 1.43281\n",
            "Resultado[26]: 1.81143\n",
            "Resultado[27]: 1.57788\n",
            "Resultado[28]: 0.522009\n",
            "Resultado[29]: 1.08958\n",
            "Resultado[30]: 0.388924\n",
            "Resultado[31]: 0.471652\n",
            "Resultado[32]: 1.44868\n",
            "Resultado[33]: 1.28042\n",
            "Resultado[34]: 0.964096\n",
            "Resultado[35]: 1.52571\n",
            "Resultado[36]: 0.628292\n",
            "Resultado[37]: 1.5072\n",
            "Resultado[38]: 0.276135\n",
            "Resultado[39]: 1.09894\n",
            "Resultado[40]: 1.05056\n",
            "Resultado[41]: 1.10509\n",
            "Resultado[42]: 0.339813\n",
            "Resultado[43]: 0.992101\n",
            "Resultado[44]: 0.60808\n",
            "Resultado[45]: 1.24063\n",
            "Resultado[46]: 0.561992\n",
            "Resultado[47]: 1.18519\n",
            "Resultado[48]: 0.556637\n",
            "Resultado[49]: 1.09598\n",
            "Resultado[50]: 1.20252\n",
            "Resultado[51]: 0.56498\n",
            "Resultado[52]: 0.795729\n",
            "Resultado[53]: 1.56708\n",
            "Resultado[54]: 0.390417\n",
            "Resultado[55]: 0.765104\n",
            "Resultado[56]: 1.19778\n",
            "Resultado[57]: 1.30297\n",
            "Resultado[58]: 0.553169\n",
            "Resultado[59]: 1.39941\n",
            "Resultado[60]: 1.37675\n",
            "Resultado[61]: 1.15825\n",
            "Resultado[62]: 1.35233\n",
            "Resultado[63]: 0.949411\n",
            "Resultado[64]: 0.80905\n",
            "Resultado[65]: 1.4297\n",
            "Resultado[66]: 0.667067\n",
            "Resultado[67]: 0.84238\n",
            "Resultado[68]: 1.06686\n",
            "Resultado[69]: 1.53466\n",
            "Resultado[70]: 1.07547\n",
            "Resultado[71]: 1.89825\n",
            "Resultado[72]: 1.16996\n",
            "Resultado[73]: 0.810841\n",
            "Resultado[74]: 0.496295\n",
            "Resultado[75]: 1.88438\n",
            "Resultado[76]: 1.8503\n",
            "Resultado[77]: 0.811788\n",
            "Resultado[78]: 1.38607\n",
            "Resultado[79]: 1.11159\n",
            "Resultado[80]: 1.80658\n",
            "Resultado[81]: 1.20669\n",
            "Resultado[82]: 1.53596\n",
            "Resultado[83]: 0.936448\n",
            "Resultado[84]: 0.458531\n",
            "Resultado[85]: 1.0601\n",
            "Resultado[86]: 0.770871\n",
            "Resultado[87]: 1.80726\n",
            "Resultado[88]: 1.15957\n",
            "Resultado[89]: 0.663964\n",
            "Resultado[90]: 0.82835\n",
            "Resultado[91]: 0.528914\n",
            "Resultado[92]: 0.590979\n",
            "Resultado[93]: 0.881269\n",
            "Resultado[94]: 1.1019\n",
            "Resultado[95]: 0.172179\n",
            "Resultado[96]: 1.53414\n",
            "Resultado[97]: 1.38214\n",
            "Resultado[98]: 0.427949\n",
            "Resultado[99]: 0.672921\n",
            "Resultado[100]: 0.789693\n",
            "Resultado[101]: 0.420485\n",
            "Resultado[102]: 0.691367\n",
            "Resultado[103]: 0.501397\n",
            "Resultado[104]: 0.845839\n",
            "Resultado[105]: 1.50681\n",
            "Resultado[106]: 1.7137\n",
            "Resultado[107]: 0.352835\n",
            "Resultado[108]: 0.376426\n",
            "Resultado[109]: 1.28174\n",
            "Resultado[110]: 0.226961\n",
            "Resultado[111]: 1.38681\n",
            "Resultado[112]: 0.832791\n",
            "Resultado[113]: 0.831203\n",
            "Resultado[114]: 1.08754\n",
            "Resultado[115]: 1.14722\n",
            "Resultado[116]: 1.23714\n",
            "Resultado[117]: 1.43779\n",
            "Resultado[118]: 0.795446\n",
            "Resultado[119]: 0.779449\n",
            "Resultado[120]: 1.10455\n",
            "Resultado[121]: 1.31547\n",
            "Resultado[122]: 1.32198\n",
            "Resultado[123]: 1.12294\n",
            "Resultado[124]: 0.902412\n",
            "Resultado[125]: 1.17673\n",
            "Resultado[126]: 0.858594\n",
            "Resultado[127]: 1.89047\n",
            "Resultado[128]: 1.3325\n",
            "Resultado[129]: 0.952125\n",
            "Resultado[130]: 1.2258\n",
            "Resultado[131]: 1.29726\n",
            "Resultado[132]: 0.667048\n",
            "Resultado[133]: 1.39145\n",
            "Resultado[134]: 1.67424\n",
            "Resultado[135]: 1.10378\n",
            "Resultado[136]: 1.45715\n",
            "Resultado[137]: 1.59704\n",
            "Resultado[138]: 0.0875597\n",
            "Resultado[139]: 1.47755\n",
            "Resultado[140]: 1.12466\n",
            "Resultado[141]: 1.11935\n",
            "Resultado[142]: 1.21783\n",
            "Resultado[143]: 0.674638\n",
            "Resultado[144]: 0.731495\n",
            "Resultado[145]: 0.970356\n",
            "Resultado[146]: 0.726107\n",
            "Resultado[147]: 1.42024\n",
            "Resultado[148]: 1.00267\n",
            "Resultado[149]: 0.855649\n",
            "Resultado[150]: 0.982581\n",
            "Resultado[151]: 0.455698\n",
            "Resultado[152]: 0.378679\n",
            "Resultado[153]: 1.21887\n",
            "Resultado[154]: 0.772724\n",
            "Resultado[155]: 0.456211\n",
            "Resultado[156]: 1.3507\n",
            "Resultado[157]: 0.880636\n",
            "Resultado[158]: 0.585375\n",
            "Resultado[159]: 1.11885\n",
            "Resultado[160]: 1.18204\n",
            "Resultado[161]: 1.27417\n",
            "Resultado[162]: 0.356822\n",
            "Resultado[163]: 1.3013\n",
            "Resultado[164]: 1.50609\n",
            "Resultado[165]: 1.0364\n",
            "Resultado[166]: 1.11638\n",
            "Resultado[167]: 0.928404\n",
            "Resultado[168]: 1.54484\n",
            "Resultado[169]: 0.961294\n",
            "Resultado[170]: 0.843696\n",
            "Resultado[171]: 0.214945\n",
            "Resultado[172]: 0.993062\n",
            "Resultado[173]: 0.762922\n",
            "Resultado[174]: 0.650854\n",
            "Resultado[175]: 0.895308\n",
            "Resultado[176]: 0.970979\n",
            "Resultado[177]: 0.741032\n",
            "Resultado[178]: 1.86388\n",
            "Resultado[179]: 0.68729\n",
            "Resultado[180]: 1.19928\n",
            "Resultado[181]: 0.858632\n",
            "Resultado[182]: 1.21578\n",
            "Resultado[183]: 0.81071\n",
            "Resultado[184]: 0.934313\n",
            "Resultado[185]: 1.07818\n",
            "Resultado[186]: 1.18951\n",
            "Resultado[187]: 1.9724\n",
            "Resultado[188]: 0.388706\n",
            "Resultado[189]: 0.761008\n",
            "Resultado[190]: 1.6171\n",
            "Resultado[191]: 0.872449\n",
            "Resultado[192]: 0.825789\n",
            "Resultado[193]: 0.915878\n",
            "Resultado[194]: 0.655813\n",
            "Resultado[195]: 1.05963\n",
            "Resultado[196]: 1.32685\n",
            "Resultado[197]: 1.17075\n",
            "Resultado[198]: 1.7053\n",
            "Resultado[199]: 0.303257\n",
            "Resultado[200]: 0.75346\n",
            "Resultado[201]: 0.417619\n",
            "Resultado[202]: 0.674563\n",
            "Resultado[203]: 1.12554\n",
            "Resultado[204]: 1.12535\n",
            "Resultado[205]: 0.102829\n",
            "Resultado[206]: 1.54286\n",
            "Resultado[207]: 0.330525\n",
            "Resultado[208]: 0.588761\n",
            "Resultado[209]: 1.05832\n",
            "Resultado[210]: 1.84957\n",
            "Resultado[211]: 1.09967\n",
            "Resultado[212]: 1.03621\n",
            "Resultado[213]: 0.719062\n",
            "Resultado[214]: 1.25187\n",
            "Resultado[215]: 0.381113\n",
            "Resultado[216]: 1.40376\n",
            "Resultado[217]: 0.948296\n",
            "Resultado[218]: 1.18876\n",
            "Resultado[219]: 1.14159\n",
            "Resultado[220]: 0.960383\n",
            "Resultado[221]: 1.05655\n",
            "Resultado[222]: 1.2197\n",
            "Resultado[223]: 0.871898\n",
            "Resultado[224]: 0.63348\n",
            "Resultado[225]: 1.05303\n",
            "Resultado[226]: 1.04816\n",
            "Resultado[227]: 0.659474\n",
            "Resultado[228]: 1.42311\n",
            "Resultado[229]: 0.825875\n",
            "Resultado[230]: 1.40085\n",
            "Resultado[231]: 0.680301\n",
            "Resultado[232]: 1.00572\n",
            "Resultado[233]: 0.88293\n",
            "Resultado[234]: 0.702433\n",
            "Resultado[235]: 1.50858\n",
            "Resultado[236]: 1.26894\n",
            "Resultado[237]: 0.166062\n",
            "Resultado[238]: 0.95221\n",
            "Resultado[239]: 1.29709\n",
            "Resultado[240]: 0.968177\n",
            "Resultado[241]: 1.06874\n",
            "Resultado[242]: 0.502818\n",
            "Resultado[243]: 1.10071\n",
            "Resultado[244]: 1.10675\n",
            "Resultado[245]: 1.6009\n",
            "Resultado[246]: 1.13357\n",
            "Resultado[247]: 0.312216\n",
            "Resultado[248]: 0.909797\n",
            "Resultado[249]: 0.333769\n",
            "Resultado[250]: 1.21967\n",
            "Resultado[251]: 0.275046\n",
            "Resultado[252]: 0.407483\n",
            "Resultado[253]: 0.977237\n",
            "Resultado[254]: 1.47907\n",
            "Resultado[255]: 0.945383\n",
            "Resultado[256]: 1.62445\n",
            "Resultado[257]: 0.463585\n",
            "Resultado[258]: 0.742337\n",
            "Resultado[259]: 1.05319\n",
            "Resultado[260]: 1.49088\n",
            "Resultado[261]: 1.35669\n",
            "Resultado[262]: 0.935292\n",
            "Resultado[263]: 1.76375\n",
            "Resultado[264]: 1.86406\n",
            "Resultado[265]: 0.457319\n",
            "Resultado[266]: 0.939867\n",
            "Resultado[267]: 0.572324\n",
            "Resultado[268]: 1.17347\n",
            "Resultado[269]: 0.707042\n",
            "Resultado[270]: 0.823889\n",
            "Resultado[271]: 1.09979\n",
            "Resultado[272]: 1.66655\n",
            "Resultado[273]: 1.57561\n",
            "Resultado[274]: 1.12767\n",
            "Resultado[275]: 0.257971\n",
            "Resultado[276]: 1.28289\n",
            "Resultado[277]: 1.243\n",
            "Resultado[278]: 1.28357\n",
            "Resultado[279]: 1.06327\n",
            "Resultado[280]: 1.10427\n",
            "Resultado[281]: 1.23135\n",
            "Resultado[282]: 1.25296\n",
            "Resultado[283]: 0.391483\n",
            "Resultado[284]: 0.353234\n",
            "Resultado[285]: 1.07897\n",
            "Resultado[286]: 0.750951\n",
            "Resultado[287]: 0.625463\n",
            "Resultado[288]: 0.742133\n",
            "Resultado[289]: 1.82638\n",
            "Resultado[290]: 1.82987\n",
            "Resultado[291]: 0.48835\n",
            "Resultado[292]: 1.69902\n",
            "Resultado[293]: 1.33094\n",
            "Resultado[294]: 0.738103\n",
            "Resultado[295]: 0.130217\n",
            "Resultado[296]: 0.914805\n",
            "Resultado[297]: 0.886504\n",
            "Resultado[298]: 1.16524\n",
            "Resultado[299]: 1.59419\n",
            "Resultado[300]: 1.37651\n",
            "Resultado[301]: 1.6956\n",
            "Resultado[302]: 0.851925\n",
            "Resultado[303]: 1.54496\n",
            "Resultado[304]: 1.52213\n",
            "Resultado[305]: 1.46386\n",
            "Resultado[306]: 1.4557\n",
            "Resultado[307]: 0.464758\n",
            "Resultado[308]: 1.68356\n",
            "Resultado[309]: 1.1373\n",
            "Resultado[310]: 0.189914\n",
            "Resultado[311]: 0.264033\n",
            "Resultado[312]: 0.629086\n",
            "Resultado[313]: 1.27248\n",
            "Resultado[314]: 0.428525\n",
            "Resultado[315]: 1.3128\n",
            "Resultado[316]: 0.539837\n",
            "Resultado[317]: 0.932501\n",
            "Resultado[318]: 1.32946\n",
            "Resultado[319]: 0.98582\n",
            "Resultado[320]: 0.848806\n",
            "Resultado[321]: 1.20359\n",
            "Resultado[322]: 1.40189\n",
            "Resultado[323]: 1.45922\n",
            "Resultado[324]: 1.46404\n",
            "Resultado[325]: 1.50711\n",
            "Resultado[326]: 1.86404\n",
            "Resultado[327]: 1.75868\n",
            "Resultado[328]: 0.921683\n",
            "Resultado[329]: 1.28745\n",
            "Resultado[330]: 0.897829\n",
            "Resultado[331]: 1.15524\n",
            "Resultado[332]: 1.60865\n",
            "Resultado[333]: 0.829428\n",
            "Resultado[334]: 0.565208\n",
            "Resultado[335]: 0.789689\n",
            "Resultado[336]: 0.264926\n",
            "Resultado[337]: 1.71852\n",
            "Resultado[338]: 1.66162\n",
            "Resultado[339]: 0.752263\n",
            "Resultado[340]: 0.77425\n",
            "Resultado[341]: 0.779617\n",
            "Resultado[342]: 1.03052\n",
            "Resultado[343]: 1.13141\n",
            "Resultado[344]: 0.56945\n",
            "Resultado[345]: 0.302411\n",
            "Resultado[346]: 1.01911\n",
            "Resultado[347]: 0.904944\n",
            "Resultado[348]: 1.5454\n",
            "Resultado[349]: 1.56586\n",
            "Resultado[350]: 0.54962\n",
            "Resultado[351]: 0.524623\n",
            "Resultado[352]: 1.34882\n",
            "Resultado[353]: 0.48136\n",
            "Resultado[354]: 1.42087\n",
            "Resultado[355]: 1.1361\n",
            "Resultado[356]: 1.493\n",
            "Resultado[357]: 0.833409\n",
            "Resultado[358]: 1.45401\n",
            "Resultado[359]: 0.475722\n",
            "Resultado[360]: 1.38368\n",
            "Resultado[361]: 0.49074\n",
            "Resultado[362]: 0.235789\n",
            "Resultado[363]: 1.03055\n",
            "Resultado[364]: 0.130056\n",
            "Resultado[365]: 0.8918\n",
            "Resultado[366]: 0.951959\n",
            "Resultado[367]: 1.49422\n",
            "Resultado[368]: 0.659561\n",
            "Resultado[369]: 1.31122\n",
            "Resultado[370]: 1.20921\n",
            "Resultado[371]: 1.27555\n",
            "Resultado[372]: 0.363043\n",
            "Resultado[373]: 0.225722\n",
            "Resultado[374]: 1.07378\n",
            "Resultado[375]: 0.840516\n",
            "Resultado[376]: 1.47958\n",
            "Resultado[377]: 0.860259\n",
            "Resultado[378]: 0.72694\n",
            "Resultado[379]: 1.49656\n",
            "Resultado[380]: 0.487221\n",
            "Resultado[381]: 0.536673\n",
            "Resultado[382]: 1.80478\n",
            "Resultado[383]: 0.739963\n",
            "Resultado[384]: 1.00648\n",
            "Resultado[385]: 1.54477\n",
            "Resultado[386]: 1.49851\n",
            "Resultado[387]: 1.91789\n",
            "Resultado[388]: 0.929283\n",
            "Resultado[389]: 0.678609\n",
            "Resultado[390]: 0.845054\n",
            "Resultado[391]: 0.460528\n",
            "Resultado[392]: 1.85038\n",
            "Resultado[393]: 1.04\n",
            "Resultado[394]: 1.2116\n",
            "Resultado[395]: 0.434318\n",
            "Resultado[396]: 0.924916\n",
            "Resultado[397]: 1.138\n",
            "Resultado[398]: 1.00189\n",
            "Resultado[399]: 0.194039\n",
            "Resultado[400]: 0.929148\n",
            "Resultado[401]: 1.22405\n",
            "Resultado[402]: 1.55914\n",
            "Resultado[403]: 1.28921\n",
            "Resultado[404]: 1.28555\n",
            "Resultado[405]: 0.491482\n",
            "Resultado[406]: 0.486945\n",
            "Resultado[407]: 1.45523\n",
            "Resultado[408]: 1.2728\n",
            "Resultado[409]: 0.914647\n",
            "Resultado[410]: 1.03179\n",
            "Resultado[411]: 1.18321\n",
            "Resultado[412]: 0.319957\n",
            "Resultado[413]: 0.686404\n",
            "Resultado[414]: 1.19917\n",
            "Resultado[415]: 1.33021\n",
            "Resultado[416]: 0.619255\n",
            "Resultado[417]: 0.555981\n",
            "Resultado[418]: 1.6962\n",
            "Resultado[419]: 0.666809\n",
            "Resultado[420]: 0.813013\n",
            "Resultado[421]: 1.11234\n",
            "Resultado[422]: 1.1103\n",
            "Resultado[423]: 0.637284\n",
            "Resultado[424]: 1.06281\n",
            "Resultado[425]: 0.867494\n",
            "Resultado[426]: 0.900897\n",
            "Resultado[427]: 1.05131\n",
            "Resultado[428]: 0.371921\n",
            "Resultado[429]: 0.542698\n",
            "Resultado[430]: 0.618153\n",
            "Resultado[431]: 1.69942\n",
            "Resultado[432]: 0.427166\n",
            "Resultado[433]: 0.415798\n",
            "Resultado[434]: 1.10059\n",
            "Resultado[435]: 1.1051\n",
            "Resultado[436]: 1.2409\n",
            "Resultado[437]: 1.21356\n",
            "Resultado[438]: 0.785023\n",
            "Resultado[439]: 1.7343\n",
            "Resultado[440]: 1.40278\n",
            "Resultado[441]: 0.942433\n",
            "Resultado[442]: 0.652501\n",
            "Resultado[443]: 1.52453\n",
            "Resultado[444]: 0.731451\n",
            "Resultado[445]: 1.59552\n",
            "Resultado[446]: 0.940135\n",
            "Resultado[447]: 1.58701\n",
            "Resultado[448]: 1.55033\n",
            "Resultado[449]: 0.490661\n",
            "Resultado[450]: 1.08542\n",
            "Resultado[451]: 1.56452\n",
            "Resultado[452]: 0.81321\n",
            "Resultado[453]: 0.677516\n",
            "Resultado[454]: 0.62893\n",
            "Resultado[455]: 0.446066\n",
            "Resultado[456]: 1.01361\n",
            "Resultado[457]: 0.335382\n",
            "Resultado[458]: 1.38224\n",
            "Resultado[459]: 1.69053\n",
            "Resultado[460]: 0.927138\n",
            "Resultado[461]: 1.4633\n",
            "Resultado[462]: 0.580979\n",
            "Resultado[463]: 1.23457\n",
            "Resultado[464]: 1.05312\n",
            "Resultado[465]: 0.782231\n",
            "Resultado[466]: 1.321\n",
            "Resultado[467]: 1.46485\n",
            "Resultado[468]: 1.12484\n",
            "Resultado[469]: 0.732442\n",
            "Resultado[470]: 0.554185\n",
            "Resultado[471]: 1.78548\n",
            "Resultado[472]: 1.56701\n",
            "Resultado[473]: 0.233901\n",
            "Resultado[474]: 0.980771\n",
            "Resultado[475]: 1.06294\n",
            "Resultado[476]: 1.21048\n",
            "Resultado[477]: 1.77518\n",
            "Resultado[478]: 0.576546\n",
            "Resultado[479]: 0.922764\n",
            "Resultado[480]: 0.734987\n",
            "Resultado[481]: 0.768206\n",
            "Resultado[482]: 1.05175\n",
            "Resultado[483]: 1.19019\n",
            "Resultado[484]: 1.37543\n",
            "Resultado[485]: 0.930361\n",
            "Resultado[486]: 1.10114\n",
            "Resultado[487]: 1.49741\n",
            "Resultado[488]: 1.8367\n",
            "Resultado[489]: 1.6174\n",
            "Resultado[490]: 0.65574\n",
            "Resultado[491]: 0.725377\n",
            "Resultado[492]: 0.206576\n",
            "Resultado[493]: 0.657669\n",
            "Resultado[494]: 1.51031\n",
            "Resultado[495]: 1.41279\n",
            "Resultado[496]: 1.28543\n",
            "Resultado[497]: 1.30128\n",
            "Resultado[498]: 1.41406\n",
            "Resultado[499]: 0.51377\n",
            "Suma total: 505.721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a realizar el mismo código pero quitando los prints para poder ver el tiempo de ejecución del programa tal y como fue solicitado para los cálculos de speedup."
      ],
      "metadata": {
        "id": "xyoB3j2b5WhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name lab5_timed.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        c[idx] = a[idx] + b[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1000000;  // Tamaño de los vectores\n",
        "    float *h_a, *h_b, *h_c; // Vectores en la CPU\n",
        "    float *d_a, *d_b, *d_c; // Vectores en la GPU\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Alojar memoria en la CPU\n",
        "    h_a = (float *)malloc(size);\n",
        "    h_b = (float *)malloc(size);\n",
        "    h_c = (float *)malloc(size);\n",
        "\n",
        "    if (h_a == nullptr || h_b == nullptr || h_c == nullptr) {\n",
        "        std::cerr << \"Error al alojar memoria en la CPU.\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Inicializar los vectores en la CPU con valores aleatorios\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        h_b[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    // Alojar memoria en la GPU\n",
        "    cudaError_t cudaStatus;\n",
        "    cudaStatus = cudaMalloc(&d_a, size);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al alojar memoria en la GPU para d_a: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "    cudaStatus = cudaMalloc(&d_b, size);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al alojar memoria en la GPU para d_b: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "    cudaStatus = cudaMalloc(&d_c, size);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al alojar memoria en la GPU para d_c: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Copiar datos desde la CPU a la GPU\n",
        "    cudaStatus = cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al copiar datos desde la CPU a la GPU para d_a: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "    cudaStatus = cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al copiar datos desde la CPU a la GPU para d_b: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Configuración de la cuadrícula y bloque\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Lanzar el kernel de CUDA\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
        "\n",
        "    cudaStatus = cudaGetLastError();\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al lanzar el kernel de CUDA: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Copiar el resultado de la GPU a la CPU\n",
        "    cudaStatus = cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"Error al copiar datos desde la GPU a la CPU para d_c: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Imprimir el vector resultante y la suma\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        sum += h_c[i];\n",
        "    }\n",
        "    std::cout << \"Suma total: \" << sum << std::endl;\n",
        "\n",
        "    // Liberar memoria\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TCYtlmiN5hi-",
        "outputId": "42741888-b0b0-48f2-da6d-ee2174efb6d8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/lab5_timed.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 /content/src/lab5_timed.cu -o \"/content/src/lab5_timed.o\""
      ],
      "metadata": {
        "id": "HjjmKshr5jHv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!chmod 755 /content/src/lab5_timed.o\n",
        "!/content/src/lab5_timed.o"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9wASqql85i_2",
        "outputId": "69cfbba1-f298-4912-a32f-cdb6cf8c81fb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suma total: 999672\n",
            "CPU times: user 13.9 ms, sys: 71 µs, total: 13.9 ms\n",
            "Wall time: 412 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya que sabemos y hemos obtenido los valores para el codigo en secuencial con cuda vamos a diseñar el formato paralelo y correrlo para obtener el tiempo del mismo y hacer los respectivos calculos."
      ],
      "metadata": {
        "id": "mVqWZDBF_9N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile secuencial.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "\n",
        "int main() {\n",
        "    int n = 1000000;  // Tamaño de los vectores\n",
        "    float *h_a, *h_b, *h_c; // Vectores en la CPU\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Alojar memoria en la CPU\n",
        "    h_a = (float *)malloc(size);\n",
        "    h_b = (float *)malloc(size);\n",
        "    h_c = (float *)malloc(size);\n",
        "\n",
        "    // Inicializar los vectores en la CPU con valores aleatorios\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        h_b[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    // Realizar la suma de los vectores en forma secuencial\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_c[i] = h_a[i] + h_b[i];\n",
        "    }\n",
        "\n",
        "    // Imprimir la suma total\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        sum += h_c[i];\n",
        "    }\n",
        "    std::cout << \"Suma total: \" << sum << std::endl;\n",
        "\n",
        "    // Liberar memoria\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MC5-USC5AEwW",
        "outputId": "a774f02a-1168-42bd-d432-8f718d254050"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting secuencial.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "g++ secuencial.cpp -o secuencial\n",
        "time ./secuencial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nZkWBB0sBg-e",
        "outputId": "5ccda758-a006-422e-d372-ed082825a48f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suma total: 999872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "real\t0m0.047s\n",
            "user\t0m0.042s\n",
            "sys\t0m0.005s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Obtener la cantidad de núcleos (cores) disponibles\n",
        "num_cores = os.cpu_count()\n",
        "\n",
        "print(\"Número de núcleos (cores) disponibles:\", num_cores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N3jIM9KeF3cL",
        "outputId": "a70e4fdd-99c0-429a-ce43-36dcc34f4796"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de núcleos (cores) disponibles: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de que tenemos el tiempo de ejecucion de ambos programas procedemos a realizar los calculos de speedup y efficiency de nuestro programa paralelo. Para ello tomaremos un tamaño de 1,000,000 y realizamos la ejecución donde obtenemos lo siguiente:\n",
        "\n",
        "\n",
        "*   Tiempo secuencial: 47 ms\n",
        "*   Tiempo paralelo: 13.9 ms\n",
        "\n",
        "* Speedup = 47 ms / 13.9 ms ≈ 3.38\n",
        "* Efficiency = 3.38 / 2 ≈ 1.69\n",
        "\n"
      ],
      "metadata": {
        "id": "b3oZUB5EBqzK"
      }
    }
  ]
}